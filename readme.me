# Chroniqueur de Runeterra : Un Agent RAG sur l'univers de League of Legends

Ce projet implÃ©mente un agent conversationnel avancÃ© basÃ© sur une architecture RAG (Retrieval-Augmented Generation). Le "Chroniqueur de Runeterra" est un chatbot spÃ©cialisÃ© dans le lore de l'univers du jeu vidÃ©o *League of Legends*. Il est capable de rÃ©pondre Ã  des questions prÃ©cises sur les champions et les rÃ©gions en se basant sur une base de connaissances construite Ã  partir de sources officielles.

## Demo

Une version live de l'application est dÃ©ployÃ©e ici : [**lo17.raphcvr.me**](https://lo17.raphcvr.me/)


## âœ¨ FonctionnalitÃ©s

  - **Interface de Chat intuitive** : Une application Streamlit permet aux utilisateurs de dialoguer en langage naturel avec l'assistant.
  - **Base de Connaissances AutomatisÃ©e** : Un script de scraping (`data_scrapper.py`) collecte automatiquement le lore depuis des sources de confiance (wiki Fandom, League of Legends Universe).
  - **Base de DonnÃ©es Vectorielle** : Utilisation de **ChromaDB** pour stocker et rechercher efficacement les documents de lore grÃ¢ce Ã  des embeddings sÃ©mantiques.
  - **Transformation de RequÃªte AvancÃ©e** : Le systÃ¨me utilise un LLM pour analyser la conversation et transformer la question de l'utilisateur en requÃªtes optimisÃ©es pour la recherche vectorielle, amÃ©liorant ainsi la pertinence des rÃ©sultats.
  - **Transparence des Sources** : Pour chaque rÃ©ponse, l'assistant cite les documents qu'il a utilisÃ©s, permettant Ã  l'utilisateur de vÃ©rifier l'information Ã  la source.
  - **Ã‰valuation Rigoureuse** : Le projet inclut un framework d'Ã©valuation (`evaluation.py`) utilisant la bibliothÃ¨que `ragas` pour mesurer la fidÃ©litÃ© (`Faithfulness`) et la correction (`Correctness`) des rÃ©ponses gÃ©nÃ©rÃ©es.
  - **GÃ©nÃ©ration de DonnÃ©es de Test** : Un script (`generate_testset.py`) permet de crÃ©er un jeu de donnÃ©es d'Ã©valuation synthÃ©tique de haute qualitÃ© pour valider la performance du RAG.
  - **PrÃªt pour le DÃ©ploiement** : L'application est entiÃ¨rement conteneurisable et fournie avec une configuration Kubernetes (`k8s-lo17-rag-app.yaml`) pour un dÃ©ploiement cloud-native.

## ğŸ›ï¸ Architecture

Le workflow de l'application suit les Ã©tapes classiques d'un pipeline RAG moderne :

1.  **Interface Utilisateur (Streamlit)** : L'utilisateur saisit sa question dans l'interface de chat.
2.  **Transformation de la RequÃªte (`inference.py`)** : Un premier appel au LLM (Google Gemini) analyse la question dans le contexte de la conversation et gÃ©nÃ¨re des requÃªtes de recherche sÃ©mantique optimisÃ©es.
3.  **RÃ©cupÃ©ration d'Information (`rag_core.py`)** : Les requÃªtes optimisÃ©es sont utilisÃ©es pour interroger la base de donnÃ©es vectorielle ChromaDB. Les documents les plus pertinents sont rÃ©cupÃ©rÃ©s.
4.  **Augmentation du Contexte** : Les documents rÃ©cupÃ©rÃ©s sont injectÃ©s dans le contexte d'un nouveau prompt.
5.  **GÃ©nÃ©ration de la RÃ©ponse (`inference.py`)** : Le prompt augmentÃ© est envoyÃ© au LLM, qui a pour instruction d'agir comme le "Chroniqueur de Runeterra" et de synthÃ©tiser une rÃ©ponse en se basant *uniquement* sur les documents fournis.
6.  **Affichage en Streaming (Streamlit)** : La rÃ©ponse est affichÃ©e en temps rÃ©el dans l'interface, et les sources sont listÃ©es dans un menu dÃ©roulant pour la transparence.

## ğŸ—‚ï¸ Structure du Projet

```
.
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ config.toml           # ThÃ¨me et configuration de l'interface Streamlit
â”œâ”€â”€ dataset_rag_lol_definitive/
â”‚   â”œâ”€â”€ knowledge_base/       # (GÃ©nÃ©rÃ©) Contient les .txt du lore scrapÃ©
â”‚   â””â”€â”€ synthetic_evaluation.csv # (GÃ©nÃ©rÃ©) Jeu de donnÃ©es pour l'Ã©valuation
â”œâ”€â”€ app.py                      # Script CLI simple pour tester le RAG
â”œâ”€â”€ create_database.py          # Script pour construire la base de donnÃ©es ChromaDB
â”œâ”€â”€ data_scrapper.py            # Script pour scraper le lore et crÃ©er la base de connaissance
â”œâ”€â”€ evaluation.py               # Script pour Ã©valuer le RAG avec Ragas
â”œâ”€â”€ generate_testset.py         # Script pour gÃ©nÃ©rer le jeu de donnÃ©es d'Ã©valuation
â”œâ”€â”€ inference.py                # Logique d'infÃ©rence du chatbot
â”œâ”€â”€ k8s-lo17-rag-app.yaml       # Fichier de dÃ©ploiement Kubernetes
â”œâ”€â”€ rag_core.py                 # CÅ“ur du systÃ¨me RAG (connexion DB, query, modÃ¨les)
â”œâ”€â”€ streamlit_app.py            # Application principale Streamlit
â”œâ”€â”€ pyproject.toml              # DÃ©pendances et configuration du projet
â””â”€â”€ ...
```

## ğŸš€ Installation et Lancement Local

Suivez ces Ã©tapes pour lancer l'application sur votre machine.

### PrÃ©requis

  - Python 3.12+
  - Un gestionnaire de paquets comme `pip` ou `uv`.

### 1\. Cloner le DÃ©pÃ´t

```bash
git clone https://github.com/Ryustel/LO17ProjetRAG.git
cd LO17ProjetRAG
```

### 2\. Installer les DÃ©pendances

Il est recommandÃ© d'utiliser un environnement virtuel Ã  l'aide d'uv ([**Installation d'uv**](https://docs.astral.sh/uv/getting-started/installation/)

Installez les dÃ©pendances listÃ©es dans `pyproject.toml` :

```bash
uv sync
```

### 3\. Configurer les ClÃ©s d'API

CrÃ©ez un fichier `.env` Ã  la racine du projet et ajoutez vos clÃ©s d'API. Le projet utilise les modÃ¨les de Google pour le RAG et potentiellement OpenAI pour la gÃ©nÃ©ration du jeu de test.

```env
# .env
GOOGLE_API_KEY="VOTRE_CLE_API_GOOGLE"
OPENAI_API_KEY="VOTRE_CLE_API_OPENAI"
```

### 4\. Construire la Base de Connaissances

Ces scripts doivent Ãªtre exÃ©cutÃ©s dans l'ordre.

```bash
# 1. Scraper les donnÃ©es du web
uv run data_scrapper.py

# 2. Construire la base de donnÃ©es vectorielle
uv run create_database.py
```

Ã€ la fin de cette Ã©tape, vous devriez avoir un dossier `database/chroma_db` peuplÃ©.

### 5\. Lancer l'Application Streamlit

```bash
uv run streamlit run streamlit_app.py
```

L'application devrait Ãªtre accessible Ã  l'adresse `http://localhost:8501`.

## ğŸ“Š Ã‰valuation du SystÃ¨me

Le projet est dotÃ© d'un systÃ¨me d'Ã©valuation pour mesurer la qualitÃ© des rÃ©ponses.

1.  **GÃ©nÃ©rer le jeu de test (Optionnel)** :
    Le script `generate_testset.py` utilise un LLM (OpenAI par dÃ©faut) pour crÃ©er des questions/rÃ©ponses pertinentes Ã  partir de la base de connaissances.

    ```bash
    python generate_testset.py
    ```

    Cela crÃ©era le fichier `dataset_rag_lol_definitive/synthetic_evaluation.csv`.

2.  **Lancer l'Ã©valuation** :
    Ce script utilise le fichier CSV gÃ©nÃ©rÃ© pour Ã©valuer le pipeline RAG sur les mÃ©triques de `faithfulness` et `answer_correctness`.

    ```bash
    python evaluation.py
    ```

    Les rÃ©sultats dÃ©taillÃ©s seront sauvegardÃ©s dans `evaluation_results.csv`.

## ğŸ“¦ DÃ©ploiement

Le fichier `k8s-lo17-rag-app.yaml` contient la configuration complÃ¨te pour un dÃ©ploiement sur un cluster Kubernetes.

Points clÃ©s :

  - **Init Container** : Un conteneur d'initialisation se charge d'exÃ©cuter `data_scrapper.py` et `create_database.py` au premier dÃ©marrage du pod.
  - **Persistance** : Un `PersistentVolumeClaim` est utilisÃ© pour que la base de donnÃ©es ChromaDB ne soit pas reconstruite Ã  chaque redÃ©marrage du pod.
  - **Secrets** : Les clÃ©s d'API doivent Ãªtre fournies au cluster via des secrets Kubernetes (`google-api-secret`, `openai-api-secret`).
  - **Ingress** : Une rÃ¨gle Ingress est dÃ©finie pour exposer le service Streamlit sur le web, par exemple via le domaine `lo17.raphcvr.me`.

## ğŸ› ï¸ Technologies UtilisÃ©es

  - **Frontend** : Streamlit
  - **Backend & Orchestration** : Python, LangChain
  - **LLMs** : Google Gemini, OpenAI (pour la crÃ©ation du jeu de test)
  - **Embeddings** : Google Generative AI Embeddings, OpenAI Embeddings (pour la crÃ©ation du jeu de test)
  - **Base de DonnÃ©es Vectorielle** : ChromaDB
  - **Scraping** : BeautifulSoup, Requests
  - **Ã‰valuation RAG** : Ragas
  - **DÃ©ploiement** : Docker, Kubernetes